{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02dcc521-e349-42ad-9749-903832f8e83c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Sequence, Callable, Tuple, Iterator, List\n",
    "from rl.distribution import Distribution, SampledDistribution, Choose, Gaussian\n",
    "from rl.markov_decision_process import MarkovDecisionProcess, \\\n",
    "    NonTerminal, State, Terminal\n",
    "from rl.function_approx import DNNSpec, AdamGradient, DNNApprox\n",
    "from rl.approximate_dynamic_programming import QValueFunctionApprox, back_opt_qvf\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class AssetAllocDiscrete:\n",
    "    risky_return_distributions: Sequence[Distribution[float]]\n",
    "    riskless_returns: Sequence[float]\n",
    "    utility_func: Callable[[float], float]\n",
    "    risky_alloc_choices: Sequence[float]\n",
    "    feature_functions: Sequence[Callable[[Tuple[float, float]], float]]\n",
    "    dnn_spec: DNNSpec\n",
    "    initial_wealth_distribution: Distribution[float]\n",
    "\n",
    "    def time_steps(self) -> int:\n",
    "        return len(self.risky_return_distributions)\n",
    "\n",
    "    def uniform_actions(self) -> Choose[float]:\n",
    "        return Choose(self.risky_alloc_choices)\n",
    "\n",
    "    def get_mdp(self, t: int) -> MarkovDecisionProcess[float, float]:\n",
    "        \"\"\"\n",
    "        State is Wealth W_t, Action is investment in risky asset (= x_t)\n",
    "        Investment in riskless asset is W_t - x_t\n",
    "        \"\"\"\n",
    "\n",
    "        distr: Distribution[float] = self.risky_return_distributions[t]\n",
    "        rate: float = self.riskless_returns[t]\n",
    "        alloc_choices: Sequence[float] = self.risky_alloc_choices\n",
    "        steps: int = self.time_steps()\n",
    "        utility_f: Callable[[float], float] = self.utility_func\n",
    "\n",
    "        class AssetAllocMDP(MarkovDecisionProcess[float, float]):\n",
    "\n",
    "            def step(\n",
    "                self,\n",
    "                wealth: NonTerminal[float],\n",
    "                alloc: float\n",
    "            ) -> SampledDistribution[Tuple[State[float], float]]:\n",
    "\n",
    "                def sr_sampler_func(\n",
    "                    wealth=wealth,\n",
    "                    alloc=alloc\n",
    "                ) -> Tuple[State[float], float]:\n",
    "                    next_wealth: float = alloc * (1 + distr.sample()) \\\n",
    "                        + (wealth.state - alloc) * (1 + rate)\n",
    "                    reward: float = utility_f(next_wealth) \\\n",
    "                        if t == steps - 1 else 0.\n",
    "                    next_state: State[float] = Terminal(next_wealth) \\\n",
    "                        if t == steps - 1 else NonTerminal(next_wealth)\n",
    "                    return (next_state, reward)\n",
    "\n",
    "                return SampledDistribution(\n",
    "                    sampler=sr_sampler_func,\n",
    "                    expectation_samples=1000\n",
    "                )\n",
    "\n",
    "            def actions(self, wealth: NonTerminal[float]) -> Sequence[float]:\n",
    "                return alloc_choices\n",
    "\n",
    "        return AssetAllocMDP()\n",
    "\n",
    "    def get_qvf_func_approx(self) -> \\\n",
    "            DNNApprox[Tuple[NonTerminal[float], float]]:\n",
    "\n",
    "        adam_gradient: AdamGradient = AdamGradient(\n",
    "            learning_rate=0.1,\n",
    "            decay1=0.9,\n",
    "            decay2=0.999\n",
    "        )\n",
    "        ffs: List[Callable[[Tuple[NonTerminal[float], float]], float]] = []\n",
    "        for f in self.feature_functions:\n",
    "            def this_f(pair: Tuple[NonTerminal[float], float], f=f) -> float:\n",
    "                return f((pair[0].state, pair[1]))\n",
    "            ffs.append(this_f)\n",
    "\n",
    "        return DNNApprox.create(\n",
    "            feature_functions=ffs,\n",
    "            dnn_spec=self.dnn_spec,\n",
    "            adam_gradient=adam_gradient\n",
    "        )\n",
    "\n",
    "    def get_states_distribution(self, t: int) -> \\\n",
    "            SampledDistribution[NonTerminal[float]]:\n",
    "\n",
    "        actions_distr: Choose[float] = self.uniform_actions()\n",
    "\n",
    "        def states_sampler_func() -> NonTerminal[float]:\n",
    "            wealth: float = self.initial_wealth_distribution.sample()\n",
    "            for i in range(t):\n",
    "                distr: Distribution[float] = self.risky_return_distributions[i]\n",
    "                rate: float = self.riskless_returns[i]\n",
    "                alloc: float = actions_distr.sample()\n",
    "                wealth = alloc * (1 + distr.sample()) + \\\n",
    "                    (wealth - alloc) * (1 + rate)\n",
    "            return NonTerminal(wealth)\n",
    "\n",
    "        return SampledDistribution(states_sampler_func)\n",
    "\n",
    "    def backward_induction_qvf(self) -> \\\n",
    "            Iterator[QValueFunctionApprox[float, float]]:\n",
    "\n",
    "        init_fa: DNNApprox[Tuple[NonTerminal[float], float]] = \\\n",
    "            self.get_qvf_func_approx()\n",
    "\n",
    "        mdp_f0_mu_triples: Sequence[Tuple[\n",
    "            MarkovDecisionProcess[float, float],\n",
    "            DNNApprox[Tuple[NonTerminal[float], float]],\n",
    "            SampledDistribution[NonTerminal[float]]\n",
    "        ]] = [(\n",
    "            self.get_mdp(i),\n",
    "            init_fa,\n",
    "            self.get_states_distribution(i)\n",
    "        ) for i in range(self.time_steps())]\n",
    "\n",
    "        num_state_samples: int = 300\n",
    "        error_tolerance: float = 1e-6\n",
    "\n",
    "        return back_opt_qvf(\n",
    "            mdp_f0_mu_triples=mdp_f0_mu_triples,\n",
    "            γ=1.0,\n",
    "            num_state_samples=num_state_samples,\n",
    "            error_tolerance=error_tolerance\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11114d84-1886-403e-9dee-92d5630b3f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(steps,r,alpha,a,b,p):\n",
    "\n",
    "    from pprint import pprint\n",
    "\n",
    "    \n",
    "    init_wealth: float = 1.0\n",
    "    init_wealth_stdev: float = 0\n",
    "    \n",
    "    \n",
    "    base_alloc: float = np.log((p*(a-r))/((p-1)*(b-r)))/alpha/(a-b)\n",
    "\n",
    "    n = 10000\n",
    "    table = [a]*int(p*n)\n",
    "    tableB = [b]*int((1-p)*n)\n",
    "    table.extend(tableB)\n",
    "    risky_ret: Sequence[Choose] = [Choose(options = table) for _ in range(steps)]\n",
    "    print(risky_ret)\n",
    "    riskless_ret: Sequence[float] = [r for _ in range(steps)]\n",
    "    utility_function: Callable[[float], float] = lambda x: - np.exp(-alpha * x) / alpha\n",
    "    alloc_choices: Sequence[float] = np.linspace(\n",
    "        2/3 * base_alloc,\n",
    "        base_alloc,\n",
    "        101\n",
    "    )\n",
    "    feature_funcs: Sequence[Callable[[Tuple[float, float]], float]] = \\\n",
    "        [\n",
    "            lambda _: 1.,\n",
    "            lambda w_x: w_x[0],\n",
    "            lambda w_x: w_x[1],\n",
    "            lambda w_x: w_x[1]\n",
    "        ]\n",
    "    dnn: DNNSpec = DNNSpec(\n",
    "        neurons=[],\n",
    "        bias=True,\n",
    "        hidden_activation=lambda x: x,\n",
    "        hidden_activation_deriv=lambda y: np.ones_like(y),\n",
    "        output_activation=lambda x: - (np.exp(-alpha * (x)))/alpha,\n",
    "        output_activation_deriv=lambda y: -y\n",
    "    )\n",
    "    init_wealth_distr: Gaussian = Gaussian(μ=init_wealth, σ=init_wealth_stdev)\n",
    "\n",
    "    aad: AssetAllocDiscrete = AssetAllocDiscrete(\n",
    "        risky_return_distributions=risky_ret,\n",
    "        riskless_returns=riskless_ret,\n",
    "        utility_func=utility_function,\n",
    "        risky_alloc_choices=alloc_choices,\n",
    "        feature_functions=feature_funcs,\n",
    "        dnn_spec=dnn,\n",
    "        initial_wealth_distribution=init_wealth_distr\n",
    "    )\n",
    "\n",
    "    it_qvf: Iterator[QValueFunctionApprox[float, float]] = \\\n",
    "        aad.backward_induction_qvf()\n",
    "\n",
    "    print(\"Backward Induction on Q-Value Function\")\n",
    "    print(\"--------------------------------------\")\n",
    "    print()\n",
    "    for t, q in enumerate(it_qvf):\n",
    "        print(f\"Time {t:d}\")\n",
    "        print()\n",
    "        opt_alloc: float = max(\n",
    "            ((q((NonTerminal(init_wealth), ac)), ac) for ac in alloc_choices),\n",
    "            key=itemgetter(0)\n",
    "        )[1]\n",
    "        val: float = max(q((NonTerminal(init_wealth), ac))\n",
    "                         for ac in alloc_choices)\n",
    "        print(f\"Opt Risky Allocation = {opt_alloc:.3f}, Opt Val = {val:.3f}\")\n",
    "        print(\"Optimal Weights below:\")\n",
    "        for wts in q.weights:\n",
    "            pprint(wts.weights)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481b2734-3584-4748-8053-ad61c1ccdc5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0.08: 0.6, 0.02: 0.4}]\n",
      "Backward Induction on Q-Value Function\n",
      "--------------------------------------\n",
      "\n",
      "Time 0\n",
      "\n",
      "Opt Risky Allocation = 33.582, Opt Val = -0.238\n",
      "Optimal Weights below:\n",
      "array([[ 0.86312549,  0.46631981, -0.00915266,  0.01224989]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T = 1\n",
    "r: float = 0.03\n",
    "alpha: float = 1.0\n",
    "p = 0.6\n",
    "a = 0.08\n",
    "b = 0.02\n",
    "\n",
    "train(T,r,alpha,a,b,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b9ee00-fbfd-4b87-9bb3-b73b243b1cc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{0.08: 0.6, 0.02: 0.4}, {0.08: 0.6, 0.02: 0.4}, {0.08: 0.6, 0.02: 0.4}, {0.08: 0.6, 0.02: 0.4}, {0.08: 0.6, 0.02: 0.4}, {0.08: 0.6, 0.02: 0.4}, {0.08: 0.6, 0.02: 0.4}, {0.08: 0.6, 0.02: 0.4}, {0.08: 0.6, 0.02: 0.4}, {0.08: 0.6, 0.02: 0.4}]\n"
     ]
    }
   ],
   "source": [
    "T = 10\n",
    "r: float = 0.03\n",
    "alpha: float = 1.0\n",
    "p = 0.6\n",
    "a = 0.08\n",
    "b = 0.02\n",
    "\n",
    "train(T,r,alpha,a,b,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e44464-7a8f-44bb-bdcf-d9c0e2e0b0e4",
   "metadata": {},
   "source": [
    "It took long time to converge but the allocation doesn't converge to the analytical solution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
