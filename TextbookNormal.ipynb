{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447a7c72-23ac-4da6-af61-52ff46737ccf",
   "metadata": {},
   "source": [
    "## Textbook solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b1e978-836e-44bd-80d1-d94965d427f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Sequence, Callable, Tuple, Iterator, List\n",
    "from rl.distribution import Distribution, SampledDistribution, Choose, Gaussian, Bernoulli\n",
    "from rl.markov_decision_process import MarkovDecisionProcess, \\\n",
    "    NonTerminal, State, Terminal\n",
    "from rl.policy import DeterministicPolicy\n",
    "from rl.function_approx import DNNSpec, AdamGradient, DNNApprox\n",
    "from rl.approximate_dynamic_programming import back_opt_vf_and_policy, \\\n",
    "    back_opt_qvf, ValueFunctionApprox, QValueFunctionApprox\n",
    "from operator import itemgetter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23c44db1-02a4-4b1c-9ad2-ec166b1b6e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class AssetAllocDiscrete:\n",
    "    risky_return_distributions: Sequence[Distribution[float]]\n",
    "    riskless_returns: Sequence[float]\n",
    "    utility_func: Callable[[float], float]\n",
    "    risky_alloc_choices: Sequence[float]\n",
    "    feature_functions: Sequence[Callable[[Tuple[float, float]], float]]\n",
    "    dnn_spec: DNNSpec\n",
    "    initial_wealth_distribution: Distribution[float]\n",
    "\n",
    "    def time_steps(self) -> int:\n",
    "        return len(self.risky_return_distributions)\n",
    "\n",
    "    def uniform_actions(self) -> Choose[float]:\n",
    "        return Choose(self.risky_alloc_choices)\n",
    "\n",
    "    def get_mdp(self, t: int) -> MarkovDecisionProcess[float, float]:\n",
    "        \"\"\"\n",
    "        State is Wealth W_t, Action is investment in risky asset (= x_t)\n",
    "        Investment in riskless asset is W_t - x_t\n",
    "        \"\"\"\n",
    "\n",
    "        distr: Distribution[float] = self.risky_return_distributions[t]\n",
    "        rate: float = self.riskless_returns[t]\n",
    "        alloc_choices: Sequence[float] = self.risky_alloc_choices\n",
    "        steps: int = self.time_steps()\n",
    "        utility_f: Callable[[float], float] = self.utility_func\n",
    "\n",
    "        class AssetAllocMDP(MarkovDecisionProcess[float, float]):\n",
    "\n",
    "            def step(\n",
    "                self,\n",
    "                wealth: NonTerminal[float],\n",
    "                alloc: float\n",
    "            ) -> SampledDistribution[Tuple[State[float], float]]:\n",
    "\n",
    "                def sr_sampler_func(\n",
    "                    wealth=wealth,\n",
    "                    alloc=alloc\n",
    "                ) -> Tuple[State[float], float]:\n",
    "                    next_wealth: float = alloc * (1 + distr.sample()) \\\n",
    "                        + (wealth.state - alloc) * (1 + rate)\n",
    "                    reward: float = utility_f(next_wealth) \\\n",
    "                        if t == steps - 1 else 0.\n",
    "                    next_state: State[float] = Terminal(next_wealth) \\\n",
    "                        if t == steps - 1 else NonTerminal(next_wealth)\n",
    "                    return (next_state, reward)\n",
    "\n",
    "                return SampledDistribution(\n",
    "                    sampler=sr_sampler_func,\n",
    "                    expectation_samples=1000\n",
    "                )\n",
    "\n",
    "            def actions(self, wealth: NonTerminal[float]) -> Sequence[float]:\n",
    "                return alloc_choices\n",
    "\n",
    "        return AssetAllocMDP()\n",
    "\n",
    "    def get_qvf_func_approx(self) -> \\\n",
    "            DNNApprox[Tuple[NonTerminal[float], float]]:\n",
    "\n",
    "        adam_gradient: AdamGradient = AdamGradient(\n",
    "            learning_rate=0.1,\n",
    "            decay1=0.9,\n",
    "            decay2=0.999\n",
    "        )\n",
    "        ffs: List[Callable[[Tuple[NonTerminal[float], float]], float]] = []\n",
    "        for f in self.feature_functions:\n",
    "            def this_f(pair: Tuple[NonTerminal[float], float], f=f) -> float:\n",
    "                return f((pair[0].state, pair[1]))\n",
    "            ffs.append(this_f)\n",
    "\n",
    "        return DNNApprox.create(\n",
    "            feature_functions=ffs,\n",
    "            dnn_spec=self.dnn_spec,\n",
    "            adam_gradient=adam_gradient\n",
    "        )\n",
    "\n",
    "    def get_states_distribution(self, t: int) -> \\\n",
    "            SampledDistribution[NonTerminal[float]]:\n",
    "\n",
    "        actions_distr: Choose[float] = self.uniform_actions()\n",
    "\n",
    "        def states_sampler_func() -> NonTerminal[float]:\n",
    "            wealth: float = self.initial_wealth_distribution.sample()\n",
    "            for i in range(t):\n",
    "                distr: Distribution[float] = self.risky_return_distributions[i]\n",
    "                rate: float = self.riskless_returns[i]\n",
    "                alloc: float = actions_distr.sample()\n",
    "                wealth = alloc * (1 + distr.sample()) + \\\n",
    "                    (wealth - alloc) * (1 + rate)\n",
    "            return NonTerminal(wealth)\n",
    "\n",
    "        return SampledDistribution(states_sampler_func)\n",
    "\n",
    "    def backward_induction_qvf(self) -> \\\n",
    "            Iterator[QValueFunctionApprox[float, float]]:\n",
    "\n",
    "        init_fa: DNNApprox[Tuple[NonTerminal[float], float]] = \\\n",
    "            self.get_qvf_func_approx()\n",
    "\n",
    "        mdp_f0_mu_triples: Sequence[Tuple[\n",
    "            MarkovDecisionProcess[float, float],\n",
    "            DNNApprox[Tuple[NonTerminal[float], float]],\n",
    "            SampledDistribution[NonTerminal[float]]\n",
    "        ]] = [(\n",
    "            self.get_mdp(i),\n",
    "            init_fa,\n",
    "            self.get_states_distribution(i)\n",
    "        ) for i in range(self.time_steps())]\n",
    "\n",
    "        num_state_samples: int = 300\n",
    "        error_tolerance: float = 1e-6\n",
    "\n",
    "        return back_opt_qvf(\n",
    "            mdp_f0_mu_triples=mdp_f0_mu_triples,\n",
    "            γ=1.0,\n",
    "            num_state_samples=num_state_samples,\n",
    "            error_tolerance=error_tolerance\n",
    "        )\n",
    "\n",
    "    def get_vf_func_approx(\n",
    "        self,\n",
    "        ff: Sequence[Callable[[NonTerminal[float]], float]]\n",
    "    ) -> DNNApprox[NonTerminal[float]]:\n",
    "\n",
    "        adam_gradient: AdamGradient = AdamGradient(\n",
    "            learning_rate=0.1,\n",
    "            decay1=0.9,\n",
    "            decay2=0.999\n",
    "        )\n",
    "        return DNNApprox.create(\n",
    "            feature_functions=ff,\n",
    "            dnn_spec=self.dnn_spec,\n",
    "            adam_gradient=adam_gradient\n",
    "        )\n",
    "\n",
    "    def backward_induction_vf_and_pi(\n",
    "        self,\n",
    "        ff: Sequence[Callable[[NonTerminal[float]], float]]\n",
    "    ) -> Iterator[Tuple[ValueFunctionApprox[float],\n",
    "                        DeterministicPolicy[float, float]]]:\n",
    "\n",
    "        init_fa: DNNApprox[NonTerminal[float]] = self.get_vf_func_approx(ff)\n",
    "\n",
    "        mdp_f0_mu_triples: Sequence[Tuple[\n",
    "            MarkovDecisionProcess[float, float],\n",
    "            DNNApprox[NonTerminal[float]],\n",
    "            SampledDistribution[NonTerminal[float]]\n",
    "        ]] = [(\n",
    "            self.get_mdp(i),\n",
    "            init_fa,\n",
    "            self.get_states_distribution(i)\n",
    "        ) for i in range(self.time_steps())]\n",
    "\n",
    "        num_state_samples: int = 300\n",
    "        error_tolerance: float = 1e-8\n",
    "\n",
    "        return back_opt_vf_and_policy(\n",
    "            mdp_f0_mu_triples=mdp_f0_mu_triples,\n",
    "            γ=1.0,\n",
    "            num_state_samples=num_state_samples,\n",
    "            error_tolerance=error_tolerance\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4750ad74-a66c-49bd-9bac-8d404a5ec1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backward Induction on Q-Value Function\n",
      "--------------------------------------\n",
      "\n",
      "Time 0\n",
      "\n",
      "Opt Risky Allocation = 1.200, Opt Val = -0.225\n",
      "Optimal Weights below:\n",
      "array([[ 0.14002606,  1.30640451,  0.07275709, -0.02942518]])\n",
      "\n",
      "Time 1\n",
      "\n",
      "Opt Risky Allocation = 1.300, Opt Val = -0.256\n",
      "Optimal Weights below:\n",
      "array([[ 0.09133244,  1.22370965,  0.06946811, -0.0263658 ]])\n",
      "\n",
      "Time 2\n",
      "\n",
      "Opt Risky Allocation = 1.400, Opt Val = -0.291\n",
      "Optimal Weights below:\n",
      "array([[ 0.04393242,  1.14459727,  0.06682979, -0.02387583]])\n",
      "\n",
      "Time 3\n",
      "\n",
      "Opt Risky Allocation = 1.500, Opt Val = -0.328\n",
      "Optimal Weights below:\n",
      "array([[-0.00215165,  1.06969654,  0.06457868, -0.02192665]])\n",
      "\n",
      "Analytical Solution\n",
      "-------------------\n",
      "\n",
      "Time 0\n",
      "\n",
      "Opt Risky Allocation = 1.224, Opt Val = -0.225\n",
      "Bias Weight = 0.135\n",
      "W_t Weight = 1.311\n",
      "x_t Weight = 0.074\n",
      "x_t^2 Weight = -0.030\n",
      "\n",
      "Time 1\n",
      "\n",
      "Opt Risky Allocation = 1.310, Opt Val = -0.257\n",
      "Bias Weight = 0.090\n",
      "W_t Weight = 1.225\n",
      "x_t Weight = 0.069\n",
      "x_t^2 Weight = -0.026\n",
      "\n",
      "Time 2\n",
      "\n",
      "Opt Risky Allocation = 1.402, Opt Val = -0.291\n",
      "Bias Weight = 0.045\n",
      "W_t Weight = 1.145\n",
      "x_t Weight = 0.064\n",
      "x_t^2 Weight = -0.023\n",
      "\n",
      "Time 3\n",
      "\n",
      "Opt Risky Allocation = 1.500, Opt Val = -0.328\n",
      "Bias Weight = 0.000\n",
      "W_t Weight = 1.070\n",
      "x_t Weight = 0.060\n",
      "x_t^2 Weight = -0.020\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "steps: int = 4\n",
    "μ: float = 0.13\n",
    "σ: float = 0.2\n",
    "r: float = 0.07\n",
    "a: float = 1.0\n",
    "init_wealth: float = 1.0\n",
    "init_wealth_stdev: float = 0.1\n",
    "\n",
    "excess: float = μ - r\n",
    "var: float = σ * σ\n",
    "base_alloc: float = excess / (a * var)\n",
    "risky_ret: Sequence[Gaussian] = [Gaussian(μ=μ, σ=σ) for _ in range(steps)]\n",
    "riskless_ret: Sequence[float] = [r for _ in range(steps)]\n",
    "utility_function: Callable[[float], float] = lambda x: - np.exp(-a * x) / a\n",
    "alloc_choices: Sequence[float] = np.linspace(\n",
    "    2 / 3 * base_alloc,\n",
    "    4 / 3 * base_alloc,\n",
    "    11\n",
    ")\n",
    "feature_funcs: Sequence[Callable[[Tuple[float, float]], float]] = \\\n",
    "    [\n",
    "        lambda _: 1.,\n",
    "        lambda w_x: w_x[0],\n",
    "        lambda w_x: w_x[1],\n",
    "        lambda w_x: w_x[1] * w_x[1]\n",
    "    ]\n",
    "dnn: DNNSpec = DNNSpec(\n",
    "    neurons=[],\n",
    "    bias=False,\n",
    "    hidden_activation=lambda x: x,\n",
    "    hidden_activation_deriv=lambda y: np.ones_like(y),\n",
    "        output_activation=lambda x: - np.sign(a) * np.exp(-x),\n",
    "        output_activation_deriv=lambda y: -y\n",
    "    )\n",
    "init_wealth_distr: Gaussian = Gaussian(μ=init_wealth, σ=init_wealth_stdev)\n",
    "\n",
    "aad: AssetAllocDiscrete = AssetAllocDiscrete(\n",
    "    risky_return_distributions=risky_ret,\n",
    "    riskless_returns=riskless_ret,\n",
    "    utility_func=utility_function,\n",
    "    risky_alloc_choices=alloc_choices,\n",
    "    feature_functions=feature_funcs,\n",
    "    dnn_spec=dnn,\n",
    "    initial_wealth_distribution=init_wealth_distr\n",
    ")\n",
    "\n",
    "it_qvf: Iterator[QValueFunctionApprox[float, float]] = \\\n",
    "    aad.backward_induction_qvf()\n",
    "\n",
    "print(\"Backward Induction on Q-Value Function\")\n",
    "print(\"--------------------------------------\")\n",
    "print()\n",
    "for t, q in enumerate(it_qvf):\n",
    "    print(f\"Time {t:d}\")\n",
    "    print()\n",
    "    opt_alloc: float = max(\n",
    "        ((q((NonTerminal(init_wealth), ac)), ac) for ac in alloc_choices),\n",
    "        key=itemgetter(0)\n",
    "    )[1]\n",
    "    val: float = max(q((NonTerminal(init_wealth), ac))\n",
    "                     for ac in alloc_choices)\n",
    "    print(f\"Opt Risky Allocation = {opt_alloc:.3f}, Opt Val = {val:.3f}\")\n",
    "    print(\"Optimal Weights below:\")\n",
    "    for wts in q.weights:\n",
    "        pprint(wts.weights)\n",
    "    print()\n",
    "\n",
    "print(\"Analytical Solution\")\n",
    "print(\"-------------------\")\n",
    "print()\n",
    "\n",
    "for t in range(steps):\n",
    "    print(f\"Time {t:d}\")\n",
    "    print()\n",
    "    left: int = steps - t\n",
    "    growth: float = (1 + r) ** (left - 1)\n",
    "    alloc: float = base_alloc / growth\n",
    "    vval: float = - np.exp(- excess * excess * left / (2 * var)\n",
    "                           - a * growth * (1 + r) * init_wealth) / a\n",
    "    bias_wt: float = excess * excess * (left - 1) / (2 * var) + \\\n",
    "        np.log(np.abs(a))\n",
    "    w_t_wt: float = a * growth * (1 + r)\n",
    "    x_t_wt: float = a * excess * growth\n",
    "    x_t2_wt: float = - var * (a * growth) ** 2 / 2\n",
    "\n",
    "    print(f\"Opt Risky Allocation = {alloc:.3f}, Opt Val = {vval:.3f}\")\n",
    "    print(f\"Bias Weight = {bias_wt:.3f}\")\n",
    "    print(f\"W_t Weight = {w_t_wt:.3f}\")\n",
    "    print(f\"x_t Weight = {x_t_wt:.3f}\")\n",
    "    print(f\"x_t^2 Weight = {x_t2_wt:.3f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
